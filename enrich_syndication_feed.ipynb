{"nbformat_minor": 1, "cells": [{"cell_type": "markdown", "metadata": {}, "source": "## Enriching Web Syndication Feeds using Watson Natural Understanding\n\nThis notebook fetches a web syndication feed and enriches it with keyword, entity and category information. The notebook source is located at https://github.com/ibm-watson-data-lab/enriching-web-syndication-feeds\n\nSetup:\n * Load [this notebook](https://raw.githubusercontent.com/ibm-watson-data-lab/enriching-web-syndication-feeds/master/enrich_syndication_feed.ipynb) into a project in [Data Science Experience](http://datascience.ibm.com/analytics) ([Instructions](https://apsportal.ibm.com/docs/content/analyze-data/creating-notebooks.html)) \n * [Provision an instance of the Watson Natural Language service in Bluemix](https://console.bluemix.net/catalog/services/natural-language-understanding/) (use the _Lite_ plan, which is free!)\n * Take note of the service credentials.\n * Customize the Syndication feed URL `feed_url` in cell 3, if desired \n  ```\n  # TODO: customize syndication feed URL\n  feed_url = 'http://feeds.wnyc.org/radiolab'\n  ```\n * Enter the credentials of your Watson Natural Language service instance in cell 4. \n  ```\n  # @hidden_cell\n  # TODO: replace with your Watson Natural Language Understanding service credentials (https://console.bluemix.net/catalog/services/natural-language-understanding/)\n  nlu_credentials = {\n      \"url\": \"https://gateway.watsonplatform.net/natural-language-understanding/api\",\n      \"username\": \"my_nlu_username\",\n      \"password\": \"my_nlu_password\"\n  }\n  ```\n  \n * Run all cells"}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "# uncomment and run this cell if any of the libraries listed in the next cells cannot be imported; restart the kernel\n#!pip install --user feedparser\n#!pip install --user pixiedust\n#!pip install --user --upgrade watson-developer-cloud", "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "import feedparser\nimport numpy\nimport pandas as pd\nimport pixiedust\nimport pprint\nimport requests\nimport urllib\nimport watson_developer_cloud\nimport watson_developer_cloud.natural_language_understanding.features.v1 as Features\nimport IPython\nfrom datetime import datetime\nfrom time import mktime\nfrom timeit import default_timer as timer", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "Customize the configuration."}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "# ---------------------------\n# configuration settings\n# ---------------------------\n# @hidden_cell\n# TODO: customize syndication feed URL\nfeed_url = 'http://feeds.wnyc.org/radiolab'\n\n# TODO: replace with your Watson Natural Language Understanding service credentials (https://console.bluemix.net/catalog/services/natural-language-understanding/)\nnlu_credentials = {\n      \"url\": \"https://gateway.watsonplatform.net/natural-language-understanding/api\",\n      \"username\": \"\",\n      \"password\": \"\"\n}\n\n# Enrich feed data with information derived using Natural Language Understanding processing (True/False)\nuse_nlu = True\n\n# log debug output (False/True)\ndebug = False\n\n# Natural Language Understanding currently supports the following languages (expressed using ISO 639-1 code):\n# \"ar\" (Arabic), \"en\" (English), \"fr\" (French), \"de\" (German), \"it\" (Italian), \"pt\" (Portuguese), \"ru\" (Russian), \"es\" (Spanish), and \"sv\" (Swedish). \n# Refer to https://www.ibm.com/watson/developercloud/doc/natural-language-understanding/index.html#supported-languages for more information\n# Set ignore_feed_language to False to enable processing of feeds in languages other than \"en\" (some NLU features might not work or return unexpected results)\nignore_feed_language = False\n\nif use_nlu and \\\n    (nlu_credentials['username'] is None or len(nlu_credentials['username'].strip()) == 0 or \\\n     nlu_credentials['password'] is None or len(nlu_credentials['password'].strip()) == 0):\n    print 'Error. Watson Natural Language Understanding credentials must be configured.'\n", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "Load the feed."}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "# load feed\nif feed_url is not None:\n    feed = feedparser.parse(feed_url)\n    if feed['status'] != 200:\n        print 'Error. The feed could not be loaded from {}. The server returned status code {}'.format(feed_url, feed['status'])\n    else:\n        print 'Feed was loaded from {}. It contains {} items.'.format(feed_url, len(feed['entries']))\n        language = feed.feed.get('language', 'en-us').split('-')[0]\n        if language != 'en':\n            print 'Warning! The feed uses language \"{}\".'.format(language)\n            if ignore_feed_language:\n                language = 'en'\n                print 'Feed language will be ignored. Using default \"en\", which might yield unexpected results.'\n            else:\n                print 'One or more Watson Natural Language Understanding features might not work. Refer to https://www.ibm.com/watson/developercloud/doc/natural-language-understanding/index.html#supported-languages for details. In the configuration settings cell set ignore_feed_language to True to use the default, which is English.'\n        \nelse:\n    print 'Error. A feed URL must be configured. Check your settings and re-run this cell.'", "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"scrolled": true, "collapsed": true}, "source": "if debug:\n    # print parsed feed entries\n    for item in feed.entries:\n            print '*' * 80\n            pprint.pprint(item)", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "Create `items_df` DataFrame representing the feed. Columns: publication year,publication month, publication month, episode title, episode tags, episode summary,  episode URL"}, {"outputs": [], "cell_type": "code", "metadata": {"scrolled": true, "collapsed": true}, "source": "summmary_property_names = ['subtitle_detail', 'summary_detail']\n# example schemes: None, 'http://www.itunes.com/'\nschemes = [None, 'http://www.itunes.com/']\n\nepisodes = []\n# extract information from every feed item\nfor item in feed.entries:\n    # construct datetime object from publication date\n    published = datetime.fromtimestamp(mktime(item['published_parsed']))\n\n    # episode title and publication info\n    episode = {\n        'title': item.get('title', item.get('subtitle', None)),\n        'published_year': published.strftime('%Y'),\n        'published_month': published.strftime('%Y-%m'),\n        'published_day': published.strftime('%Y-%m-%d')\n    }\n    \n    # episode URL is either\n    # (1) a web page for the specific episode or\n    # (2) a direct link to the media (e.g. mp3 file)\n    # (3) None\n    url = item.get('link')\n    if url is None:\n        # no episode web page was specified; try getting the media URL\n        # multiple media URLs might be specified (e.g. if media is available in different formats)\n        mc = item.get('media_content', [])\n        for c in mc:\n            url = c.get('url')\n            if url is not None:\n                break\n    episode['url'] = url\n    \n    # episode summary\n    summary = episode['title']\n    for pname in summmary_property_names:\n        if item.get(pname) is not None:\n            if len(item[pname].get('value','').strip()) > 0:\n                summary = item[pname].get('value')\n                break\n    episode['summary'] = summary\n    \n    # episode tags\n    if item.get('tags') is not None:\n        tags = []\n        for tag in item.get('tags'):\n            if len(schemes) == 0 or tag['scheme'] in schemes:\n               tags.append(tag['term'])\n        tags.sort()\n        episode['tags'] = ','.join(tags)\n    else:\n        episode['tags'] = None\n        \n    episodes.append(episode)\n\n# create DataFrame\ncols = ['published_year','published_month', 'published_day', 'title', 'tags', 'summary', 'url']\nitems_df = pd.DataFrame(episodes, columns = cols)\n\nif debug:\n    print 'DataFrame shape:\\t{}'.format(items_df.shape)\n    print 'DataFrame structure:\\n{}'.format(items_df.dtypes)\n\nIPython.display.display(items_df.head(5))", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "Enrich `items_df` DataFrame with keyword, category, and entity information."}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "#\n# enrich syndication feed by running Natural Language Understanding analysis for each item:\n# - add categories for each item\n# - add entities for each item\n# - add keywords for each item\n#\nif use_nlu:\n    \n    # Apply optional entity filter; refer to https://www.ibm.com/watson/developercloud/natural-language-understanding/api/v1/?python#entities for details\n    # Examples: apply no filter; all entity types will be returned: [] \n    #           only identify companies, people and organizations: []'Company', 'Person', 'Organization']\n    entity_filter = ['Company', 'Person', 'Organization']\n    \n    # Categorize content into a 5-level taxonomy. The top three categories will be returned as a CSV string. (https://www.ibm.com/watson/developercloud/natural-language-understanding/api/v1/?python#categories)\n    # category list: https://www.ibm.com/watson/developercloud/doc/natural-language-understanding/categories.html\n    items_df['categories'] = None\n    \n    # Identify people, cities, organizations, and many other types of entities (https://www.ibm.com/watson/developercloud/natural-language-understanding/api/v1/?python#entities)\n    # entitiy types and subtypes: https://www.ibm.com/watson/developercloud/doc/natural-language-understanding/entity-types.html\n    items_df['entities'] = None\n    \n    # Identify the important keywords (https://www.ibm.com/watson/developercloud/natural-language-understanding/api/v1/?python#keywords)\n    items_df['keywords'] = None   \n    \n    try: \n        nlu = watson_developer_cloud.NaturalLanguageUnderstandingV1(version='2017-02-27',\n                                                                    username = nlu_credentials['username'],\n                                                                    password = nlu_credentials['password'])\n        \n        def call_nlu(summary):\n            ''' Send request to Watson NLU and fetch categories, entities and keywords\n                Input: string to be analyzed. This string can contain HTML.\n                Output: raw NLU response (Dict)\n            '''\n            \n            if debug:\n                start = timer()\n            else:\n                print '\\b.',\n            try:\n                response = nlu.analyze(html = summary, \\\n                                       features = [Features.Categories(), Features.Entities(), Features.Keywords()], \\\n                                       language = language)\n                if debug:\n                    end = timer()\n                    print 'NLU summary processing completed in {:.1f} seconds.'.format(end - start)\n            except watson_developer_cloud.WatsonException as e:\n                print u'\\nError. Watson Natural Language Understanding processing for \"{}\" failed: {}'.format(summary, e)\n                response = {}\n            return response\n\n        def getKeywords(response):\n            ''' Extract keywords from NLU response\n                Input: raw NLU response\n                Output: array of keywords, formatted as a csv string: \"keyword1,...,keywordN\" or None\n            '''\n            out = []\n            for keyword in response.get('keywords', []):\n                # {u'relevance': 0.941254, u'text': u'pediatrician Nadine Burke'}\n                out.append(keyword['text'])\n            if len(out) > 0:\n                return ','.join(out)\n            return None\n\n        def getEntities(response, *args):\n            ''' Extract entities from NLU response\n                Input: raw NLU response\n                Input: Tuple containing explicitly white-listed entity types. If no white-list is provided all entity types will be captured\n                Output: array of entities, formatted as a dict {'entity_type1': ['entity1',...], 'entity_type2': [...]} or None\n            '''\n            ignore_filter = len(args) == 0\n            out = {}\n            for entity in response.get('entities', []):\n                # {u'count': 1, u'relevance': 0.974444, u'text': u'Nadine Burke Harris',  u'type': u'Person'}\n                if ignore_filter or entity['type'] in args:\n                    if out.get(entity['type']) is None:\n                        out[entity['type']] = []\n                    out[entity['type']].append(entity['text'])\n                elif debug:\n                    print 'Skipping entity {} because it is not white-listed.'.format(entity['type'])\n            if len(out) > 0:\n                return out\n            else:\n                return None\n\n        def getCategories(response):\n            ''' Extract categories from NLU response\n                Input: raw NLU response\n                Output: array of categories, formatted as a csv string: \"category1,...,categoryN\" or None\n            '''\n            out = []\n            for category in response.get('categories', []):\n                # {u'label': u'/science/biology', u'score': 0.578503}\n                out.append(category['label'])\n            if len(out) > 0:        \n                return ','.join(out)\n            return None   \n    \n        print 'Running Natural Language Understanding analysis for {} feed items'.format(len(items_df.index))\n        nlu_start = timer()\n        items_df['raw_nlu_response'] = items_df['summary'].apply(call_nlu)\n        nlu_end = timer()\n        print '\\nNLU analysis completed in {:.1f} seconds'.format(nlu_end - nlu_start)\n        \n        items_df['keywords'] = items_df['raw_nlu_response'].apply(getKeywords)\n        items_df['entities'] = items_df['raw_nlu_response'].apply(getEntities,args = ('Company','Person','Organization'))\n        items_df['categories'] = items_df['raw_nlu_response'].apply(getCategories)\n        \n    except watson_developer_cloud.WatsonException as e:\n        print u'Error. Watson Natural Language Understanding processing failed: {}'.format(e)\n        ", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "Inspect the enriched `items_df` DataFrame. If NLU analysis completed successfully the following new columns were added:\n - `categories` ([Details](https://www.ibm.com/watson/developercloud/doc/natural-language-understanding/categories.html))\n - `entities` ([Details](https://www.ibm.com/watson/developercloud/doc/natural-language-understanding/entity-types.html))\n - `keywords`\n - `raw_nlu_response` (NLU response details)"}, {"outputs": [], "cell_type": "code", "metadata": {"scrolled": true, "collapsed": true}, "source": "IPython.display.display(items_df.head(5))", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "### Prepare for tag analysis\n\nCreate two analysis DataFrames:\n - `tag_associations_df`: associations between tags and episodes. Columns: tag, episode title, publication year, publication month, publication day, episode URL\n - `tag_df`: summarized tag information for basic data exploration. Columns: tag, publication year, publication month, occurence per month"}, {"outputs": [], "cell_type": "code", "metadata": {"pixiedust": {"displayParams": {"valueFields": "count", "charttype": "grouped", "keyFields": "tag", "rendererId": "matplotlib", "rowCount": "500", "sortby": "Values DESC", "aggregation": "SUM", "orientation": "vertical", "chartsize": "69", "stretch": "true", "handlerId": "barChart"}}, "collapsed": true}, "source": "# create tag dataframe\ntag_associations = []\nfor row in items_df.itertuples():\n    if row[5] is not None:\n        for tag in row[5].split(','):\n            tag = tag.strip()\n            tag_associations.append((tag, row[4], row[1], row[2], row[3], row[7]))     \n\ntag_associations_df = pd.DataFrame(tag_associations, columns=['tag','title','published_year', 'published_month','published_day','url'])\nIPython.display.display(tag_associations_df.head(2))\n\n# tags by year and month\ntag_df = pd.DataFrame(tag_associations_df.groupby(['tag', 'published_year', 'published_month']).size().sort_values(ascending = False), columns = ['count']).reset_index()\nIPython.display.display(tag_df.head(5))\n\nif tag_df.size > 0:\n    display(tag_df)\nelse:\n    print 'No tags to display'", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "### Prepare for keyword analysis\n\nCreate two DataFrames:\n - `keyword_associations_df`: associations between extracted keywords and episodes. Columns: keyword, episode title, publication year, publication month, publication day, episode URL\n - `keyword_df`: summarized monthly keyword information for basic data exploration. Columns: keyword, publication year, publication month, occurence per per month"}, {"outputs": [], "cell_type": "code", "metadata": {"pixiedust": {"displayParams": {"legend": "true", "valueFields": "count", "charttype": "stacked", "keyFields": "keyword", "rowCount": "500", "sortby": "Values DESC", "aggregation": "SUM", "orientation": "vertical", "chartsize": "60", "stretch": "true", "handlerId": "barChart"}}, "collapsed": true}, "source": "if 'keywords' in items_df.columns:\n    # create keyword dataframe\n    keyword_associations = []\n    for row in items_df.itertuples():\n        if row[10] is not None:\n            for keyword in row[10].split(','):\n                keyword = keyword.strip()\n                keyword_associations.append((keyword, row[4], row[1], row[2], row[3], row[7]))     \n\n    keyword_associations_df = pd.DataFrame(keyword_associations, columns=['keyword','title','published_year', 'published_month','published_day','url'])\n    IPython.display.display(keyword_associations_df.head(2))\n    # keywords by year and month\n    keyword_df = pd.DataFrame(keyword_associations_df.groupby(['keyword', 'published_year', 'published_month']).size().sort_values(ascending = False), columns = ['count']).reset_index()\n    IPython.display.display(keyword_df.head(2))\n    if keyword_df.size > 0:\n        display(keyword_df)\n    else:\n        print 'No keywords to display'    \nelse:\n    keyword_associations_df = None\n    keyword_df = None\n    print 'The source does not contain keyword information.'", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "### Prepare for category analysis\n\nCreate two DataFrames:\n - `category_associations_df`: associations between derived categories and episodes. Columns: category, episode title, publication year, publication month, publication day, episode URL\n - `category_df`: summarized monthly category information for basic data exploration. Columns: category, publication year, publication month, occurence per per month"}, {"outputs": [], "cell_type": "code", "metadata": {"pixiedust": {"displayParams": {"valueFields": "count", "keyFields": "category", "rowCount": "500", "sortby": "Values DESC", "aggregation": "SUM", "orientation": "vertical", "chartsize": "50", "stretch": "true", "handlerId": "barChart"}}, "collapsed": true}, "source": "if 'categories' in items_df.columns:\n    # create category dataframe\n    category_associations = []\n    for row in items_df.itertuples():\n        if row[8] is not None:\n            for category in row[8].split(','):\n                category = category.strip()\n                category_associations.append((category, row[4], row[1], row[2], row[3], row[7]))     \n\n    category_associations_df = pd.DataFrame(category_associations, columns=['category','title','published_year', 'published_month','published_day','url'])\n    IPython.display.display(category_associations_df.head(2))\n    # categories by year and month\n    category_df = pd.DataFrame(category_associations_df.groupby(['category', 'published_year', 'published_month']).size().sort_values(ascending = False), columns = ['count']).reset_index()\n    IPython.display.display(category_df.head(2))\n    if category_df.size > 0:\n        display(category_df)\n    else:\n        print 'No categories to display'   \nelse:\n    category_associations_df = None\n    category_df = None\n    print 'The source does not contain category information.'", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "### Prepare for entity analysis\n\nExtract entities (people, organizations, ...) and try to associate them with Wikipedia entries to provide context information.\n\nCreate two DataFrames:\n - `entity_associations_df`: associations between identified entities and episodes. Columns: entity type, entity value, episode title, publication year, publication month, publication day, episode URL, wikipedia URL\n - `entity_df`: summarized monthly entity information for basic data exploration. Columns: entity type, entity value, publication year, publication month, occurence per per month"}, {"outputs": [], "cell_type": "code", "metadata": {"pixiedust": {"displayParams": {"legend": "false", "valueFields": "count", "keyFields": "entity_value", "rendererId": "matplotlib", "rowCount": "500", "sortby": "Values DESC", "aggregation": "SUM", "chartsize": "70", "stretch": "true", "handlerId": "barChart"}}, "collapsed": true}, "source": "if 'entities' in items_df.columns:\n    # validate wikipedia URL\n    exact_url_only = True\n\n    def getWikipediaURL(entity, exact_match_only = False):\n        url = 'https://en.wikipedia.org/w/index.php?{}'.format(urllib.urlencode({'search': entity},'utf-8'))\n        r = requests.head(url)\n        print '\\b.',\n        if r.status_code == 302:\n            return r.headers['Location']\n        elif exact_match_only:\n            return None\n        else:\n            return url    \n\n    print 'Looking up entities on Wikipedia ' \n    # create entity dataframe and lookup entities on Wikipedia\n    entity_associations = []\n    for row in items_df.itertuples():\n        if row[9] is not None:\n            for entity_type in row[9].keys():\n                for entity_value in row[9][entity_type]:\n                    entity_associations.append((entity_type, entity_value, row[4], row[1], row[2], row[3], row[7], getWikipediaURL(entity_value, exact_url_only)))     \n    \n    entity_associations_df = pd.DataFrame(entity_associations, columns=['entity_type','entity_value', 'title','published_year', 'published_month','published_day','url', 'wikipedia_url'])\n    IPython.display.display(entity_associations_df.head(2))\n    # entities by year and month\n    groupBy = ['entity_type', 'entity_value', 'published_year', 'published_month']\n    entity_df = pd.DataFrame(entity_associations_df.groupby(groupBy).size().sort_values(ascending = False), columns = ['count']).reset_index()\n    IPython.display.display(entity_df.head(200))\n    if entity_df.size > 0:\n        display(entity_df)\n    else:\n        print 'No entities to display'   \nelse:\n    entity_associations_df = None\n    entity_df = None\n    print 'The source does not contain entity information.'", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "Display all entities that were mentioned in the podcast"}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "for url in entity_associations_df.get('wikipedia_url',pd.Series([])).unique():\n    if url is not None:\n        print url", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "***"}], "metadata": {"kernelspec": {"display_name": "Python 2 with Spark 2.0", "name": "python2-spark20", "language": "python"}, "language_info": {"mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 2}, "pygments_lexer": "ipython2", "version": "2.7.11", "name": "python", "file_extension": ".py", "nbconvert_exporter": "python"}}, "nbformat": 4}