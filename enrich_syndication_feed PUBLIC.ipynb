{"nbformat_minor": 1, "cells": [{"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "import feedparser\nimport numpy\nimport pandas as pd\nimport pixiedust\nimport requests\nimport urllib\nimport IPython\nfrom datetime import datetime\nfrom time import mktime", "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "# configuration settings\nfeed_url = 'https://changelog.com/podcast/feed'\n\n# Enrich feed data with information derived using Natural Language Understanding processing (True/False)\nuse_nlu = True\n\n# log debug output (False/True)\ndebug = True", "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "# @hidden_cell\n# TODO: replace with your Watson Natural Language Understanding service credentials (https://console.bluemix.net/catalog/services/natural-language-understanding/)\nnlu_credentials = {\n      \"url\": \"https://gateway.watsonplatform.net/natural-language-understanding/api\",\n      \"username\": \"\",\n      \"password\": \"\"\n}\n\nif use_nlu and (nlu_credentials['username'] is None or nlu_credentials['password'] is None):\n    print 'Error. Watson Natural Language Understanding credentials must be configured.'", "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "# load feed\nif feed_url is not None:\n    feed = feedparser.parse(feed_url)\n    if feed['status'] != 200:\n        print 'Error. The feed could not be loaded from {}. The server returned status code {}'.format(feed_url, feed['status'])\n    else:\n        print 'Feed was loaded from {}. It contains {} items.'.format(feed_url, len(feed['entries']))\nelse:\n    print 'Error. A feed URL must be configured. Check your settings.'", "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"scrolled": true, "collapsed": true}, "source": "if debug:\n    for item in feed.entries:\n            print item", "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"scrolled": true, "collapsed": true}, "source": "summmary_property_names = ['subtitle_detail', 'summary_detail']\n# example schemes: None, 'http://www.itunes.com/'\nschemes = [None, 'http://www.itunes.com/']\n\nepisodes = []\n\nfor item in feed.entries:\n    # construct datetime object from publication date\n    published = datetime.fromtimestamp(mktime(item['published_parsed']))\n\n    # \n    episode = {\n        'title': item.get('title', item.get('subtitle', None)),\n        'url': item['link'],\n        'published_year': published.strftime('%Y'),\n        'published_month': published.strftime('%Y-%m'),\n        'published_day': published.strftime('%Y-%m-%d')\n    }\n    \n    summary = episode['title']\n    \n    for pname in summmary_property_names:\n        if item.get(pname) is not None:\n            if len(item[pname].get('value','').strip()) > 0:\n                summary = item[pname].get('value')\n                break\n\n    episode['summary'] = summary\n    \n    if item.get('tags') is not None:\n        tags = []\n        for tag in item.get('tags'):\n            if len(schemes) == 0 or tag['scheme'] in schemes:\n               tags.append(tag['term'])\n        tags.sort()\n        episode['tags'] = ','.join(tags)\n    else:\n        episode['tags'] = None\n        \n    episodes.append(episode)\n\n# create DataFrame\ncols = ['published_year','published_month', 'published_day', 'title', 'tags', 'summary', 'url']\nitems_df = pd.DataFrame(episodes, columns = cols)\n\nif debug:\n    print 'DataFrame size:\\t{}'.format(items_df.size)\n    print 'DataFrame structure:\\n{}'.format(items_df.dtypes)\n\nIPython.display.display(items_df.head(10))\n    ", "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "#\n# enrich syndication feed by running Natural Language Understanding analysis for each item:\n# - add categories for each item\n# - add entities for each item\n# - add keywords for each item\n#\nif use_nlu:\n    \n    # Apply optional entity filter; refer to https://www.ibm.com/watson/developercloud/natural-language-understanding/api/v1/?python#entities for details\n    # Examples: apply no filter; all entity types will be returned: [] \n    #           only identify companies, people and organizations: []'Company', 'Person', 'Organization']\n    entity_filter = ['Company', 'Person', 'Organization']\n    \n    # Categorize content into a 5-level taxonomy. The top three categories will be returned as a CSV string. (https://www.ibm.com/watson/developercloud/natural-language-understanding/api/v1/?python#categories)\n    # category list: https://www.ibm.com/watson/developercloud/doc/natural-language-understanding/categories.html\n    items_df['categories'] = None\n    \n    # Identify people, cities, organizations, and many other types of entities (https://www.ibm.com/watson/developercloud/natural-language-understanding/api/v1/?python#entities)\n    # entitiy types and subtypes: https://www.ibm.com/watson/developercloud/doc/natural-language-understanding/entity-types.html\n    items_df['entities'] = None\n    \n    # Identify the important keywords (https://www.ibm.com/watson/developercloud/natural-language-understanding/api/v1/?python#keywords)\n    items_df['keywords'] = None   \n    \n    import watson_developer_cloud\n    import watson_developer_cloud.natural_language_understanding.features.v1 as Features\n    from timeit import default_timer as timer\n    \n    try: \n        nlu = watson_developer_cloud.NaturalLanguageUnderstandingV1(version='2017-02-27',\n                                                                    username = nlu_credentials['username'],\n                                                                    password = nlu_credentials['password'])\n        \n        def call_nlu(summary):\n            ''' Send request to Watson NLU and fetch categories, entities and keywords\n                Input: string to be analyzed. This string can contain HTML.\n                Output: raw NLU response (Dict)\n            '''\n            \n            if debug:\n                start = timer()\n            else:\n                print '\\b.',\n            try:\n                response = nlu.analyze(html = summary, features = [Features.Categories(), Features.Entities(), Features.Keywords()])\n                if debug:\n                    end = timer()\n                    print 'NLU summary processing completed in {:.1f} seconds.'.format(end - start)\n            except watson_developer_cloud.WatsonException as e:\n                print u'\\nError. Watson Natural Language Understanding processing for \"{}\" failed: {}'.format(summary, e)\n                response = {}\n            return response\n\n        def getKeywords(response):\n            ''' Extract keywords from NLU response\n                Input: raw NLU response\n                Output: array of keywords, formatted as a csv string: \"keyword1,...,keywordN\" or None\n            '''\n            out = []\n            for keyword in response.get('keywords', []):\n                # {u'relevance': 0.941254, u'text': u'pediatrician Nadine Burke'}\n                out.append(keyword['text'])\n            if len(out) > 0:\n                return ','.join(out)\n            return None\n\n        def getEntities(response, *args):\n            ''' Extract entities from NLU response\n                Input: raw NLU response\n                Input: Tuple containing explicitly white-listed entity types. If no white-list is provided all entity types will be captured\n                Output: array of entities, formatted as a dict {'entity_type1': ['entity1',...], 'entity_type2': [...]} or None\n            '''\n            ignore_filter = len(args) == 0\n            out = {}\n            for entity in response.get('entities', []):\n                # {u'count': 1, u'relevance': 0.974444, u'text': u'Nadine Burke Harris',  u'type': u'Person'}\n                if ignore_filter or entity['type'] in args:\n                    if out.get(entity['type']) is None:\n                        out[entity['type']] = []\n                    out[entity['type']].append(entity['text'])\n                elif debug:\n                    print 'Skipping entity {} because it is not white-listed.'.format(entity['type'])\n            if len(out) > 0:\n                return out\n            else:\n                return None\n\n        def getCategories(response):\n            ''' Extract categories from NLU response\n                Input: raw NLU response\n                Output: array of categories, formatted as a csv string: \"category1,...,categoryN\" or None\n            '''\n            out = []\n            for category in response.get('categories', []):\n                # {u'label': u'/science/biology', u'score': 0.578503}\n                out.append(category['label'])\n            if len(out) > 0:        \n                return ','.join(out)\n            return None   \n    \n        print 'Running Natural Language Understanding analysis '\n        nlu_start = timer()\n        items_df['raw_nlu_response'] = items_df['summary'].apply(call_nlu)\n        nlu_end = timer()\n        print '\\nNLU analysis completed in {:.1f} seconds'.format(nlu_end - nlu_start)\n        \n        items_df['keywords'] = items_df['raw_nlu_response'].apply(getKeywords)\n        items_df['entities'] = items_df['raw_nlu_response'].apply(getEntities,args = ('Company','Person','Organization'))\n        items_df['categories'] = items_df['raw_nlu_response'].apply(getCategories)\n        \n    except watson_developer_cloud.WatsonException as e:\n        print u'Error. Watson Natural Language Understanding processing failed: {}'.format(e)    ", "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "IPython.display.display(items_df.head(5))", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "### Prepare for tag analysis"}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "# create tag dataframe\ntag_associations = []\nfor row in items_df.itertuples():\n    if row[5] is not None:\n        for tag in row[5].split(','):\n            tag = tag.strip()\n            tag_associations.append((tag, row[4], row[1], row[2], row[3], row[7]))     \n\ntag_associations_df = pd.DataFrame(tag_associations, columns=['tag','title','published_year', 'published_month','published_day','url'])\ntag_associations_df.head(2)", "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "# tags by year and month\ntag_df = pd.DataFrame(tag_associations_df.groupby(['tag', 'published_year', 'published_month']).size().sort_values(ascending = False), columns = ['count']).reset_index()\ntag_df.head(5)", "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"pixiedust": {"displayParams": {"valueFields": "count", "charttype": "stacked", "keyFields": "tag", "rowCount": "500", "sortby": "Values DESC", "aggregation": "SUM", "chartsize": "50", "stretch": "true", "handlerId": "barChart"}}, "scrolled": true, "collapsed": true}, "source": "if tag_df.size > 0:\n    display(tag_df)\nelse:\n    print 'No tags to display'", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "### Prepare for keyword analysis"}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "if 'keywords' in items_df.columns:\n    # create keyword dataframe\n    keyword_associations = []\n    for row in items_df.itertuples():\n        if row[10] is not None:\n            for keyword in row[10].split(','):\n                keyword = keyword.strip()\n                keyword_associations.append((keyword, row[4], row[1], row[2], row[3], row[7]))     \n\n    keyword_associations_df = pd.DataFrame(keyword_associations, columns=['keyword','title','published_year', 'published_month','published_day','url'])\n    IPython.display.display(keyword_associations_df.head(2))\n    # keywords by year and month\n    keyword_df = pd.DataFrame(keyword_associations_df.groupby(['keyword', 'published_year', 'published_month']).size().sort_values(ascending = False), columns = ['count']).reset_index()\n    IPython.display.display(keyword_df.head(2))\nelse:\n    keyword_associations_df = None\n    keyword_df = None\n    print 'The source does not contain keyword information.'", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "### Prepare for category analysis"}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "if 'categories' in items_df.columns:\n    # create category dataframe\n    category_associations = []\n    for row in items_df.itertuples():\n        if row[8] is not None:\n            for category in row[8].split(','):\n                category = category.strip()\n                category_associations.append((category, row[4], row[1], row[2], row[3], row[7]))     \n\n    category_associations_df = pd.DataFrame(category_associations, columns=['category','title','published_year', 'published_month','published_day','url'])\n    IPython.display.display(category_associations_df.head(2))\n    # categories by year and month\n    category_df = pd.DataFrame(category_associations_df.groupby(['category', 'published_year', 'published_month']).size().sort_values(ascending = False), columns = ['count']).reset_index()\n    IPython.display.display(category_df.head(2))\nelse:\n    category_associations_df = None\n    category_df = None\n    print 'The source does not contain category information.'", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "### Prepare for entity analysis\n\nExtract entities (people, organizations, ...) and try to associate them with Wikipedia entries to provide context information.\u00df"}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "if 'entities' in items_df.columns:\n    # validate wikipedia URL\n    exact_url_only = True\n\n    def getWikipediaURL(entity, exact_match_only = False):\n        url = 'https://en.wikipedia.org/w/index.php?{}'.format(urllib.urlencode({'search': entity_value},'utf-8'))\n        r = requests.head(url)\n        print '\\b.',\n        if r.status_code == 302:\n            return r.headers['Location']\n        elif exact_match_only:\n            return None\n        else:\n            return url    \n\n    print 'Looking up entities on Wikipedia ' \n    # create entity dataframe and lookup entities on Wikipedia\n    entity_associations = []\n    for row in items_df.itertuples():\n        if row[9] is not None:\n            for entity_type in row[9].keys():\n                for entity_value in row[9][entity_type]:\n                    entity_associations.append((entity_type, entity_value, row[4], row[1], row[2], row[3], row[7], getWikipediaURL(entity_value, exact_url_only)))     \n    \n    entity_associations_df = pd.DataFrame(entity_associations, columns=['entity_type','entity_value', 'title','published_year', 'published_month','published_day','url', 'wikipedia_url'])\n    IPython.display.display(entity_associations_df.head(2))\n    # entities by year and month\n    groupBy = ['entity_type', 'entity_value', 'published_year', 'published_month']\n    entity_df = pd.DataFrame(entity_associations_df.groupby(groupBy).size().sort_values(ascending = False), columns = ['count']).reset_index()\n    IPython.display.display(entity_df.head(2))\nelse:\n    entity_associations_df = None\n    entity_df = None\n    print 'The source does not contain entity information.'", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "***"}], "metadata": {"kernelspec": {"display_name": "Python 2 with Spark 2.0", "name": "python2-spark20", "language": "python"}, "language_info": {"mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 2}, "pygments_lexer": "ipython2", "version": "2.7.11", "name": "python", "file_extension": ".py", "nbconvert_exporter": "python"}}, "nbformat": 4}